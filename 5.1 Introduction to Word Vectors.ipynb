{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"colab":{"name":"Copy of 5.1 Introduction to Word Vectors.ipynb","provenance":[{"file_id":"1PvdpixDVeZiiEbK2pdQaAwB1SQIyrQgH","timestamp":1573397230688}]}},"cells":[{"cell_type":"markdown","metadata":{"id":"Lo6--yCcFeyD","colab_type":"text"},"source":["# Word Vectors in Python with gensim\n","\n","This notebook shows an example of how to use word vectors in Python with gensim."]},{"cell_type":"markdown","metadata":{"id":"tUNtF-G6FeyJ","colab_type":"text"},"source":["## Load Libraries\n","\n","We're using a new library called `gensim`.  It's a great library for modeling text and comes with pre-trained models that you can easily use in other contexts."]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-01-20T00:07:48.246163Z","start_time":"2019-01-20T00:07:46.748980Z"},"id":"BhvSJETNFeyN","colab_type":"code","colab":{}},"source":["%matplotlib inline\n","import numpy as np\n","import gensim\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","from collections import defaultdict"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mG0SBtX6FeyX","colab_type":"text"},"source":["### Cleaning Our Corpus\n","\n","(corpus is another name for the set of documents under consideration)"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-01-20T00:07:48.265144Z","start_time":"2019-01-20T00:07:48.249322Z"},"id":"U78OdGE6Feya","colab_type":"code","outputId":"7631b262-cfc6-4c5e-d89a-0a127582c986","executionInfo":{"status":"ok","timestamp":1572731243154,"user_tz":240,"elapsed":1102,"user":{"displayName":"Brian","photoUrl":"https://lh4.googleusercontent.com/-y5a64nLFbc4/AAAAAAAAAAI/AAAAAAAAAXs/4RRMVlUcGtY/s64/photo.jpg","userId":"11680685692091329827"}},"colab":{"base_uri":"https://localhost:8080/","height":434}},"source":["documents = [\n","    \"Perfect is the enemy of good.\",\n","    \"I'm still learning.\",\n","    \"Life is a journey, not a destination.\",\n","    \"Learning is not attained by chance, it must be sought for with ardor and attended to with diligence.\",\n","    \"Yesterday I was clever, so I changed the world. Today I am wise, so I am changing myself.\",\n","    \"Be curious, not judgmental.\",\n","    \"You don't have to be great to start, but you have to start to be great.,\"\n","    \"Be stubborn about your goals and flexible about your methods.\",\n","    \"Nothing will work unless you do.\",\n","    \"Never give up on a dream just because of the time it will take to accomplish it. The time will pass anyway.\",\n","    \"Anyone who stops learning is old, whether at twenty or eighty.\",\n","    \"Tell me and I forget. Teach me and I remember. Involve me and I learn.\",\n","    \"Change is the end result of all true learning.\",\n","    \"Live as if you were to die tomorrow. Learn as if you were to live forever.\",\n","    \"A learning curve is essential to growth.\",\n","]\n","\n","# remove common words and tokenize\n","stop_words = set('for a of the and to in'.split())\n","texts = [[word for word in document.lower().replace(\"'\", \"\").replace(\".\", \"\").split() if word not in stop_words]\n","         for document in documents]\n","\n","## remove words that appear only once\n","frequency = defaultdict(int)\n","for text in texts:\n","    for token in text:\n","        frequency[token] += 1\n","\n","texts = [[token for token in text if frequency[token] > 1]\n","         for text in texts]\n","texts"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[['is'],\n"," ['learning'],\n"," ['is', 'not'],\n"," ['learning', 'is', 'not', 'it', 'be', 'with', 'with'],\n"," ['i', 'so', 'i', 'i', 'am', 'so', 'i', 'am'],\n"," ['be', 'not'],\n"," ['you', 'have', 'be', 'you', 'have', 'be', 'about', 'your', 'about', 'your'],\n"," ['will', 'you'],\n"," ['time', 'it', 'will', 'it', 'time', 'will'],\n"," ['learning', 'is'],\n"," ['me', 'i', 'me', 'i', 'me', 'i', 'learn'],\n"," ['is', 'learning'],\n"," ['live',\n","  'as',\n","  'if',\n","  'you',\n","  'were',\n","  'learn',\n","  'as',\n","  'if',\n","  'you',\n","  'were',\n","  'live'],\n"," ['learning', 'is']]"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"AzV1F5nqFeyj","colab_type":"text"},"source":["### Creating our Word2Vec Model\n","\n","`gensim` makes it easy to train a Word2Vec model.  All training requires is passing in the corpus."]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-01-20T00:07:48.314818Z","start_time":"2019-01-20T00:07:48.268115Z"},"id":"Wk-yATt6Feyl","colab_type":"code","outputId":"f9925175-8948-4efe-b4b4-73be628dc54e","executionInfo":{"status":"ok","timestamp":1572731243156,"user_tz":240,"elapsed":1092,"user":{"displayName":"Brian","photoUrl":"https://lh4.googleusercontent.com/-y5a64nLFbc4/AAAAAAAAAAI/AAAAAAAAAXs/4RRMVlUcGtY/s64/photo.jpg","userId":"11680685692091329827"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["model = gensim.models.Word2Vec(texts, size=10, window=2, min_count=1)\n","model"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<gensim.models.word2vec.Word2Vec at 0x7ffad4bcb940>"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-01-20T00:07:48.326029Z","start_time":"2019-01-20T00:07:48.318240Z"},"id":"_-i1FfX8Feys","colab_type":"code","outputId":"1180c385-db23-4b65-b598-3d610b7114e6","executionInfo":{"status":"ok","timestamp":1572731243157,"user_tz":240,"elapsed":1082,"user":{"displayName":"Brian","photoUrl":"https://lh4.googleusercontent.com/-y5a64nLFbc4/AAAAAAAAAAI/AAAAAAAAAXs/4RRMVlUcGtY/s64/photo.jpg","userId":"11680685692091329827"}},"colab":{"base_uri":"https://localhost:8080/","height":104}},"source":["model['live']"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n","  \"\"\"Entry point for launching an IPython kernel.\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["array([ 0.04093517, -0.00830318, -0.03466499, -0.03480424, -0.00121745,\n","       -0.01612454,  0.02507611, -0.02705088,  0.01547491,  0.02742862],\n","      dtype=float32)"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"vQSRHPLJFeyz","colab_type":"text"},"source":["And we can find the most similar words too.  Obviously, our dataset is too small and we won't find anything too interesting."]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-01-20T00:07:48.341900Z","start_time":"2019-01-20T00:07:48.329322Z"},"id":"az8FYssqFey1","colab_type":"code","outputId":"d9938c11-62f8-405b-c57b-878d9badb829","executionInfo":{"status":"ok","timestamp":1572731243160,"user_tz":240,"elapsed":1072,"user":{"displayName":"Brian","photoUrl":"https://lh4.googleusercontent.com/-y5a64nLFbc4/AAAAAAAAAAI/AAAAAAAAAXs/4RRMVlUcGtY/s64/photo.jpg","userId":"11680685692091329827"}},"colab":{"base_uri":"https://localhost:8080/","height":280}},"source":["model.most_similar('live')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n","  \"\"\"Entry point for launching an IPython kernel.\n","/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n","  if np.issubdtype(vec.dtype, np.int):\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["[('time', 0.4510110020637512),\n"," ('me', 0.3452457785606384),\n"," ('as', 0.27253618836402893),\n"," ('with', 0.2470661848783493),\n"," ('will', 0.20179931819438934),\n"," ('have', 0.17683294415473938),\n"," ('learning', 0.13080498576164246),\n"," ('if', 0.12683644890785217),\n"," ('were', 0.0977015271782875),\n"," ('learn', 0.017691776156425476)]"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"0a_7KuYYFezG","colab_type":"text"},"source":["### Loading an existing corpus\n","\n","We can load some existing text and train a model on it.  In this case, we're going to use `text8` which is a small subset of Wikipedia (31MB).  See: https://github.com/RaRe-Technologies/gensim-data"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-01-20T00:07:48.533077Z","start_time":"2019-01-20T00:07:48.345058Z"},"id":"1wc2Gi_rFezI","colab_type":"code","outputId":"ae49ca15-afc9-4c82-ad33-73b03bc18d80","executionInfo":{"status":"ok","timestamp":1572731254108,"user_tz":240,"elapsed":12004,"user":{"displayName":"Brian","photoUrl":"https://lh4.googleusercontent.com/-y5a64nLFbc4/AAAAAAAAAAI/AAAAAAAAAXs/4RRMVlUcGtY/s64/photo.jpg","userId":"11680685692091329827"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["import gensim.downloader as api\n","\n","# This could be slow to download...\n","corpus = api.load(\"text8\")\n","corpus"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[==================================================] 100.0% 31.6/31.6MB downloaded\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<text8.Dataset at 0x7ffad46f2438>"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-01-20T00:09:11.533094Z","start_time":"2019-01-20T00:07:48.536503Z"},"id":"1MJQDoLYFezO","colab_type":"code","outputId":"64fb900a-8ebe-422b-8497-b38c39f390cb","executionInfo":{"status":"ok","timestamp":1572731411355,"user_tz":240,"elapsed":169240,"user":{"displayName":"Brian","photoUrl":"https://lh4.googleusercontent.com/-y5a64nLFbc4/AAAAAAAAAAI/AAAAAAAAAXs/4RRMVlUcGtY/s64/photo.jpg","userId":"11680685692091329827"}},"colab":{"base_uri":"https://localhost:8080/","height":89}},"source":["# Using small numbers here, probably want to use a bigger corpus, bigger dimensions, and more iterations.\n","model = gensim.models.Word2Vec(corpus, size=10, window=2, iter=5, min_count=1)\n","model"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["<gensim.models.word2vec.Word2Vec at 0x7ffad46ea588>"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"Sr_QsuclFezU","colab_type":"text"},"source":["We can get slightly better results (but we really should be using a much bigger corpus)"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-01-20T00:09:11.580898Z","start_time":"2019-01-20T00:09:11.536816Z"},"id":"aRD9XLxMFezV","colab_type":"code","outputId":"e570ddfa-14bc-4f8a-a404-7cc3bc3b4276","executionInfo":{"status":"ok","timestamp":1572731411359,"user_tz":240,"elapsed":169234,"user":{"displayName":"Brian","photoUrl":"https://lh4.googleusercontent.com/-y5a64nLFbc4/AAAAAAAAAAI/AAAAAAAAAXs/4RRMVlUcGtY/s64/photo.jpg","userId":"11680685692091329827"}},"colab":{"base_uri":"https://localhost:8080/","height":280}},"source":["model.most_similar(\"queen\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n","  \"\"\"Entry point for launching an IPython kernel.\n","/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n","  if np.issubdtype(vec.dtype, np.int):\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["[('tsar', 0.9781045913696289),\n"," ('king', 0.9707283973693848),\n"," ('roosevelt', 0.9589150547981262),\n"," ('alsacian', 0.9491715431213379),\n"," ('nixdorff', 0.9476499557495117),\n"," ('prince', 0.9449954032897949),\n"," ('churchill', 0.9445421695709229),\n"," ('hadrian', 0.9420450925827026),\n"," ('vampyr', 0.9411798119544983),\n"," ('maccabess', 0.9398282170295715)]"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-01-20T00:09:11.593176Z","start_time":"2019-01-20T00:09:11.584425Z"},"id":"M6hKtEnJFeza","colab_type":"code","outputId":"fe6d42ca-9f57-4e5d-c4ce-be5a9af804d4","executionInfo":{"status":"ok","timestamp":1572731411361,"user_tz":240,"elapsed":169226,"user":{"displayName":"Brian","photoUrl":"https://lh4.googleusercontent.com/-y5a64nLFbc4/AAAAAAAAAAI/AAAAAAAAAXs/4RRMVlUcGtY/s64/photo.jpg","userId":"11680685692091329827"}},"colab":{"base_uri":"https://localhost:8080/","height":104}},"source":["model['queen']"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n","  \"\"\"Entry point for launching an IPython kernel.\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["array([ 2.6956127 ,  1.2827659 ,  2.3695884 ,  0.6383235 , -2.2322483 ,\n","        0.37769857,  2.3656695 ,  0.7270439 ,  3.1721518 ,  0.6553371 ],\n","      dtype=float32)"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"moUaHD6YFezg","colab_type":"text"},"source":["### Loading a pre-trained model\n","\n","We can also use the gensim to automatically download and load a pre-trained model, or alternatively load it from disk.  Since the pre-trained model has much more data, the vectors encode some semantic meaning."]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-01-20T00:09:36.003788Z","start_time":"2019-01-20T00:09:11.596968Z"},"id":"RyXl3oWpFezi","colab_type":"code","outputId":"dc490868-a8df-435b-bade-beda9ae4c4d5","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Glove is another word embedding that uses a slightly different technique than word2vec, \n","# however, it has the same properties and API\n","model = api.load(\"glove-wiki-gigaword-50\")\n","model"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[============================----------------------] 56.8% 37.5/66.0MB downloaded"],"name":"stdout"}]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-01-20T00:09:36.494141Z","start_time":"2019-01-20T00:09:36.006827Z"},"id":"YVHcUl7rFezn","colab_type":"code","colab":{}},"source":["model.most_similar('queen')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"inBzV93YFezr","colab_type":"text"},"source":["Alternatively, we can load the same model directly from disk (the previous calls cache the files in `~/gensim-data/`."]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-01-20T00:10:00.578988Z","start_time":"2019-01-20T00:09:36.525757Z"},"id":"hj3GX9K8Fezt","colab_type":"code","colab":{}},"source":["model = gensim.models.KeyedVectors.load_word2vec_format('~/gensim-data/glove-wiki-gigaword-50/glove-wiki-gigaword-50.gz')\n","model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-01-20T00:10:00.847470Z","start_time":"2019-01-20T00:10:00.582233Z"},"id":"l56WPbt-Fezz","colab_type":"code","colab":{}},"source":["model.most_similar('queen')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-01-20T00:10:00.859049Z","start_time":"2019-01-20T00:10:00.850204Z"},"id":"oVsR7xchFez3","colab_type":"code","colab":{}},"source":["model['queen']"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CiZbTr2tFez8","colab_type":"text"},"source":["### Visualizing Words with T-SNE\n","\n","As we saw in the slides, we can visualize the distance between words using T-SNE."]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-01-20T00:10:01.154138Z","start_time":"2019-01-20T00:10:00.865386Z"},"id":"iRgJC7v6Fez8","colab_type":"code","colab":{}},"source":["from sklearn.manifold import TSNE"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-01-20T00:10:01.180821Z","start_time":"2019-01-20T00:10:01.159492Z"},"scrolled":true,"id":"m8EUNGogFe0A","colab_type":"code","colab":{}},"source":["# Gather a listing of random words\n","words = ['queen', 'princess', 'dog', 'king', 'cat', 'obama', 'clinton', 'president', 'math', 'brian']\n","\n","vecs = np.array([model[w] for w in words])\n","vecs"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-01-20T00:10:01.508727Z","start_time":"2019-01-20T00:10:01.184298Z"},"id":"efH8UDWbFe0F","colab_type":"code","colab":{}},"source":["vecs_tsne = TSNE(n_components=2, perplexity=3).fit_transform(vecs)\n","vecs_tsne"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-01-20T00:10:01.801307Z","start_time":"2019-01-20T00:10:01.512004Z"},"id":"vsczS2VtFe0J","colab_type":"code","colab":{}},"source":["ax = sns.scatterplot(vecs_tsne[:, 0], vecs_tsne[:, 1], s=100)\n","sns.set(font_scale=1.5)\n","[ax.text(p[0], p[1]+10, word, color='black') for word, p in zip(words, vecs_tsne)]\n","pass"],"execution_count":0,"outputs":[]}]}